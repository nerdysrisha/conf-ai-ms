"""
Service LLM pour l'interaction avec phi-2.7 via Ollama
"""
import os
import re
from typing import List, Dict, Any, Optional
import requests
import json
import logging

logger = logging.getLogger(__name__)

class LLMService:
    """Service pour l'interaction avec le mod√®le phi-2.7 via Ollama"""
    
    def __init__(self, endpoint: str, api_key: str, model_name: str = "phi:2.7", encrypted_search_service=None, base_url: str = "http://localhost:8000"):
        """
        Initialise le service LLM Ollama
        
        Args:
            endpoint: Endpoint Ollama (ex: http://4.232.232.104:11434)
            api_key: Cl√© API (peut √™tre ignor√©e pour Ollama local)
            model_name: Nom du mod√®le Ollama (ex: phi:2.7)
            encrypted_search_service: Service de recherche chiffr√©e (optionnel)
            base_url: URL de base de l'application pour les liens de t√©l√©chargement
        """
        # Construire l'endpoint correct pour Ollama
        if not endpoint.endswith('/'):
            endpoint += '/'
        self.chat_endpoint = endpoint + "v1/chat/completions"
        self.generate_endpoint = endpoint + "api/generate"
        
        self.model_name = model_name
        self.api_key = api_key
        self.encrypted_search_service = encrypted_search_service
        self.use_encrypted_search = encrypted_search_service is not None
        self.base_url = base_url  # Stocker l'URL de base
        
        # Headers pour Ollama (g√©n√©ralement pas d'auth requise)
        self.headers = {
            "Content-Type": "application/json"
        }
        
        # Ajouter l'auth seulement si n√©cessaire
        if api_key and api_key != "test-phi35-key":
            self.headers["Authorization"] = f"Bearer {api_key}"
        
        # Contr√¥le du debug verbose
        self.debug_verbose = os.getenv('LLM_DEBUG_VERBOSE', 'false').lower() == 'true'
        self.debug_streaming = os.getenv('LLM_DEBUG_STREAMING', 'false').lower() == 'true'
        
        logger.info(f"Service LLM Ollama initialis√© - Endpoint: {self.chat_endpoint}, Mod√®le: {model_name}")
        if self.use_encrypted_search:
            logger.info("Service de recherche chiffr√©e activ√©")
        if not self.debug_verbose:
            logger.info("Mode debug verbose d√©sactiv√©")
        if not self.debug_streaming:
            logger.info("Mode debug streaming d√©sactiv√©")
        
    def generate_response(self, 
                         messages: List[Dict[str, str]], 
                         max_tokens: int = None,
                         temperature: float = None,
                         top_p: float = None,
                         system_prompt: Optional[str] = None) -> str:
        """
        G√©n√®re une r√©ponse √† partir des messages via Ollama
        
        Args:
            messages: Liste des messages de conversation
            max_tokens: Nombre maximum de tokens (utilise Config.MAX_TOKENS si None)
            temperature: Temp√©rature pour la g√©n√©ration (utilise Config.TEMPERATURE si None)
            top_p: Param√®tre top_p pour la g√©n√©ration (utilise Config.TOP_P si None)
            system_prompt: Prompt syst√®me optionnel
            
        Returns:
            str: R√©ponse g√©n√©r√©e par le mod√®le
        """
        try:
            # Utiliser les valeurs de configuration par d√©faut si non sp√©cifi√©es
            from ..config import Config
            if max_tokens is None:
                max_tokens = Config.MAX_TOKENS
            if temperature is None:
                temperature = Config.TEMPERATURE
            if top_p is None:
                top_p = Config.TOP_P
            # Pr√©parer les messages pour Ollama
            formatted_messages = []
            
            if system_prompt:
                formatted_messages.append({
                    "role": "system",
                    "content": system_prompt
                })
            
            formatted_messages.extend(messages)
            
            # Payload pour l'API OpenAI compatible (vLLM)
            payload = {
                "model": self.model_name,
                "messages": formatted_messages,
                "stream": False,  # R√©ponse compl√®te (pour √©viter erreurs)
                "max_tokens": max_tokens if max_tokens > 0 else None,
                "temperature": temperature,
                "top_p": top_p
            }
            
            # üîç LOGGING D√âTAILL√â DU PROMPT ENVOY√â √Ä PHI-4 (conditionnel)
            if self.debug_verbose:
                print("\n" + "="*80)
                print("ü§ñ APPEL √Ä PHI-4 VIA vLLM/OpenAI")
                print("="*80)
                print(f"üì° Endpoint: {self.chat_endpoint}")
                print(f"üîß Mod√®le: {self.model_name}")
                print(f"üå°Ô∏è Temp√©rature: {temperature}")
                print(f"üìù Nombre de messages: {len(formatted_messages)}")
                
                # üìã AFFICHAGE JSON COMPLET DU PAYLOAD
                print("\nüìã PAYLOAD JSON COMPLET ENVOY√â √Ä PHI-4:")
                print("-" * 60)
                print(json.dumps(payload, indent=2, ensure_ascii=False))
                print("-" * 60)
                
                # üì° INFORMATIONS DE LA REQU√äTE HTTP
                print(f"\nüì° D√âTAILS DE LA REQU√äTE HTTP:")
                print(f"  - URL: {self.chat_endpoint}")
                print(f"  - Method: POST")
                print(f"  - Headers: {json.dumps(self.headers, indent=2, ensure_ascii=False)}")
                print(f"  - Timeout: 120 secondes")
                print("-" * 60)
                
                print("\nüìã PROMPT COMPLET ENVOY√â:")
                print("-" * 60)
                for i, msg in enumerate(formatted_messages):
                    role = msg.get('role', 'unknown')
                    content = msg.get('content', '')
                    print(f"[{i+1}] ROLE: {role.upper()}")
                    print(f"CONTENU: {content}")
                    print("-" * 40)
            
            # Debug simple : Payload LLM (conditionnel)
            if self.debug_verbose:
                print(f"ü§ñ LLM CALL ‚Üí phi-4 (NON-STREAMING)")
                print(f"üì§ INPUT: {json.dumps(payload, ensure_ascii=False)}")
                print(f"‚ùå ATTENTION: Utilisation de la m√©thode NON-STREAMING generate_response()")
                print("-" * 80)
            
            # Envoyer la requ√™te √† vLLM/OpenAI
            response = requests.post(
                self.chat_endpoint,
                headers=self.headers,
                json=payload,
                timeout=120  # Ollama peut √™tre lent
            )
            
            response.raise_for_status()
            
            # Extraire la r√©ponse Ollama
            response_data = response.json()
            
            # Debug simple : R√©ponse LLM
            print(f"üì• OUTPUT: {json.dumps(response_data, ensure_ascii=False)}")
            print("-" * 60)
            
            # Format de r√©ponse OpenAI
            if "choices" in response_data and len(response_data["choices"]) > 0:
                choice = response_data["choices"][0]
                if "message" in choice and "content" in choice["message"]:
                    response_content = choice["message"]["content"]
                    print(f"\n‚úÖ CONTENU EXTRAIT: {response_content}")
                    
                    # Statistiques OpenAI/vLLM
                    usage = response_data.get('usage', {})
                    if usage:
                        print(f"\nüìä STATISTIQUES:")
                        print(f"  - Tokens de prompt: {usage.get('prompt_tokens', 'N/A')}")
                        print(f"  - Tokens g√©n√©r√©s: {usage.get('completion_tokens', 'N/A')}")
                        print(f"  - Total tokens: {usage.get('total_tokens', 'N/A')}")
                    
                    print("="*80 + "\n")
                    return response_content
            
            print(f"‚ùå FORMAT INATTENDU: {response_data}")
            print("="*80 + "\n")
            raise ValueError(f"Format de r√©ponse OpenAI inattendu: {response_data}")
                
        except requests.exceptions.RequestException as e:
            logger.error(f"Erreur de connexion vLLM/OpenAI: {e}")
            return f"Erreur de connexion au mod√®le phi-4. V√©rifiez que vLLM est d√©marr√© et que le mod√®le {self.model_name} est disponible."
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration de r√©ponse: {e}")
            return f"Erreur lors de la g√©n√©ration de r√©ponse: {str(e)}"
    
    def generate_response_stream(self, 
                               messages: List[Dict[str, str]], 
                               max_tokens: int = None,
                               temperature: float = None,
                               top_p: float = None,
                               system_prompt: Optional[str] = None):
        """
        G√©n√®re une r√©ponse en streaming √† partir des messages via Ollama
        
        Args:
            messages: Liste des messages de conversation
            max_tokens: Nombre maximum de tokens (utilise Config.MAX_TOKENS si None)
            temperature: Temp√©rature pour la g√©n√©ration (utilise Config.TEMPERATURE si None)
            top_p: Param√®tre top_p pour la g√©n√©ration (utilise Config.TOP_P si None)
            system_prompt: Prompt syst√®me optionnel
            
        Yields:
            str: Fragments de r√©ponse g√©n√©r√©s par le mod√®le
        """
        if self.debug_streaming:
            print(f"üåäüåäüåä ENTR√âE DANS generate_response_stream() - STREAMING ACTIV√â!")
            print(f"üìã Messages: {len(messages)}")
        
        try:
            # Utiliser les valeurs de configuration par d√©faut si non sp√©cifi√©es
            from ..config import Config
            if max_tokens is None:
                max_tokens = Config.MAX_TOKENS
            if temperature is None:
                temperature = Config.TEMPERATURE
            if top_p is None:
                top_p = Config.TOP_P
                
            # Pr√©parer les messages pour Ollama
            formatted_messages = []
            
            if system_prompt:
                formatted_messages.append({
                    "role": "system",
                    "content": system_prompt
                })
            
            formatted_messages.extend(messages)
            
            # Payload pour l'API OpenAI compatible (vLLM) avec streaming activ√©
            payload = {
                "model": self.model_name,
                "messages": formatted_messages,
                "stream": True,  # Streaming activ√©
                "max_tokens": max_tokens if max_tokens > 0 else None,
                "temperature": temperature,
                "top_p": top_p
            }
            
            logger.info(f"üåä Streaming LLM call √† {self.chat_endpoint} avec {len(formatted_messages)} messages")
            if self.debug_streaming:
                print(f"üåä LLM STREAMING CALL ‚Üí phi-4 (vLLM/OpenAI)")
                print(f"üì§ STREAMING INPUT: {json.dumps(payload, ensure_ascii=False)}")
                print(f"‚úÖ STREAMING ACTIV√â: stream=True")
                print("-" * 80)
            
            # Envoyer la requ√™te √† Ollama en streaming
            response = requests.post(
                self.chat_endpoint,
                headers=self.headers,
                json=payload,
                timeout=120,
                stream=True  # Streaming HTTP activ√©
            )
            
            response.raise_for_status()
            
            # Traiter la r√©ponse en streaming OpenAI (SSE format)
            for line in response.iter_lines():
                if line:
                    try:
                        # D√©coder la ligne
                        line_text = line.decode('utf-8')
                        
                        # Ignorer les lignes qui ne sont pas des donn√©es SSE
                        if not line_text.startswith('data: '):
                            continue
                            
                        # Extraire le JSON apr√®s "data: "
                        json_str = line_text[6:]  # Supprimer "data: "
                        
                        # Ignorer la ligne de fin "[DONE]"
                        if json_str.strip() == '[DONE]':
                            break
                            
                        # Parser le JSON
                        chunk_data = json.loads(json_str)
                        
                        # Extraire le contenu du chunk OpenAI
                        if "choices" in chunk_data and len(chunk_data["choices"]) > 0:
                            choice = chunk_data["choices"][0]
                            if "delta" in choice and "content" in choice["delta"]:
                                content = choice["delta"]["content"]
                                if content:  # Ne yield que si il y a du contenu
                                    if self.debug_streaming:
                                        logger.debug(f"Chunk envoy√©: '{content}'")
                                    yield content
                            
                            # V√©rifier si c'est la fin du stream
                            if choice.get("finish_reason") is not None:
                                break
                            
                    except json.JSONDecodeError as e:
                        logger.warning(f"Erreur de parsing JSON chunk: {e}")
                        logger.warning(f"Ligne probl√©matique: {line}")
                        continue
                        
        except requests.exceptions.RequestException as e:
            logger.error(f"Erreur de connexion Ollama streaming: {e}")
            yield f"Erreur de connexion au mod√®le phi-2.7. V√©rifiez que Ollama est d√©marr√© et que le mod√®le {self.model_name} est install√©."
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration de r√©ponse streaming: {e}")
            yield f"Erreur lors de la g√©n√©ration de r√©ponse: {str(e)}"
    
    def generate_rag_response(self, 
                            user_query: str, 
                            context_documents: List[Dict[str, Any]],
                            conversation_history: Optional[List[Dict[str, str]]] = None,
                            query_embedding: Optional[List[float]] = None) -> str:
        """
        G√©n√®re une r√©ponse RAG bas√©e sur les documents de contexte
        
        Args:
            user_query: Question de l'utilisateur
            context_documents: Documents de contexte r√©cup√©r√©s
            conversation_history: Historique de conversation optionnel
            query_embedding: Embedding de la question (optionnel pour debug)
            
        Returns:
            str: R√©ponse g√©n√©r√©e avec contexte
        """
        print(f"‚ùå‚ùå‚ùå ATTENTION! ENTR√âE DANS generate_rag_response() - M√âTHODE NON-STREAMING!")
        print(f"üìã Question: {user_query[:50]}...")
        print(f"üìÑ Documents: {len(context_documents)}")
        print(f"üîç Cette m√©thode ne devrait PAS √™tre appel√©e!")
        
        try:
            # Construire le contexte √† partir des documents
            context_text = self._build_context(context_documents)
            
            # Cr√©er le prompt syst√®me pour RAG optimis√© pour phi3.5:3.8b
            system_prompt = f"""You are a professional AI assistant specialized in document analysis and information retrieval. You provide accurate, well-structured answers based on provided documents.

CORE INSTRUCTIONS:
1. **Source-based answers**: Answer ONLY using information from the provided context documents
2. **Document references**: Do not reference documents in your answer
3. **Language consistency**: Always respond in the same language as the user's question
4. **Document list**: Always end with a complete list of referenced documents

CRITICAL LINK FORMAT:
- Each document in context shows: "Source: filename.ext (ID: complete_file_id)"
- Use the COMPLETE ID exactly as provided
- Format: [filename.ext]({self.base_url}/api/files/complete_file_id/decrypt)
- Example: If context shows "Source: role_library.pdf (ID: ebcc2e60-8a86-4b2f-9c7d-1234567890ab)"
  Write: [role_library.pdf]({self.base_url}/api/files/ebcc2e60-8a86-4b2f-9c7d-1234567890ab/decrypt)

RESPONSE STRUCTURE:
1. Provide a clear, comprehensive answer with natural document references
2. End with relevance assessment:

---
### üìä √âvaluation de la r√©ponse
**Note de pertinence** : [Score]/10  
**Sources consult√©es** : [Number] document(s)

### üìÑ Documents consult√©s
**Complete IDs**: Always use the COMPLETE ID provided in the context (do not truncate)
[List each unique document referenced with download link, no duplicates]
- [filename1.ext]({self.base_url}/api/files/complete_id1/decrypt)
- [filename2.ext]({self.base_url}/api/files/complete_id2/decrypt)

DOCUMENT CONTEXT:
{context_text}

CRITICAL: Use COMPLETE IDs exactly as provided in context - never truncate or modify them!
"""
            
            # Pr√©parer les messages
            messages = []
            
            # Ajouter l'historique de conversation si fourni
            if conversation_history:
                messages.extend(conversation_history[-6:])  # Garder les 6 derniers √©changes
            
            # Ajouter la question actuelle
            messages.append({
                "role": "user",
                "content": user_query
            })
            
            print(f"\nüìã HISTORIQUE CONVERSATION: {len(conversation_history) if conversation_history else 0} messages")
            print("üîç" * 40)
            
            # G√©n√©rer la r√©ponse avec param√®tres configurables
            response = self.generate_response(
                messages=messages,
                system_prompt=system_prompt,
                max_tokens=None,  # Utiliser Config.MAX_TOKENS
                temperature=None,  # Utiliser Config.TEMPERATURE
                top_p=None  # Utiliser Config.TOP_P
            )
            
            print("üîç" * 40)
            print("‚úÖ FIN G√âN√âRATION RAG")
            print("üîç" * 40 + "\n")
            
            return response
            
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration de r√©ponse RAG: {e}")
            raise
    
    def generate_rag_response_stream(self, 
                                   user_query: str, 
                                   context_documents: List[Dict[str, Any]],
                                   conversation_history: Optional[List[Dict[str, str]]] = None,
                                   query_embedding: Optional[List[float]] = None):
        """
        G√©n√®re une r√©ponse RAG en streaming bas√©e sur les documents de contexte
        
        Args:
            user_query: Question de l'utilisateur
            context_documents: Documents de contexte r√©cup√©r√©s
            conversation_history: Historique de conversation optionnel
            query_embedding: Embedding de la question (optionnel pour debug)
            
        Yields:
            str: Fragments de r√©ponse g√©n√©r√©s avec contexte
        """
        print(f"üåäüåäüåä ENTR√âE DANS generate_rag_response_stream() - STREAMING ACTIV√â")
        print(f"üìã Question: {user_query[:50]}...")
        print(f"üìÑ Documents: {len(context_documents)}")
        
        try:
            # Construire le contexte √† partir des documents
            context_text = self._build_context(context_documents)
            
            # Cr√©er le prompt syst√®me pour RAG optimis√© pour phi3.5:3.8b
            system_prompt = f"""You are a professional AI assistant specialized in document analysis and information retrieval. You provide accurate, well-structured answers based on provided documents.

CORE INSTRUCTIONS:
1. **Source-based answers**: Answer ONLY using information from the provided context documents
2. **Document references**: Do not reference documents in your answer
3. **Language consistency**: Always respond in the same language as the user's question
4. **Document list**: Always end with a complete list of referenced documents

CRITICAL LINK FORMAT:
- Each document in context shows: "Source: filename.ext (ID: complete_file_id)"
- Use the COMPLETE ID exactly as provided
- Format: [filename.ext]({self.base_url}/api/files/complete_file_id/decrypt)
- Example: If context shows "Source: role_library.pdf (ID: ebcc2e60-8a86-4b2f-9c7d-1234567890ab)"
  Write: [role_library.pdf]({self.base_url}/api/files/ebcc2e60-8a86-4b2f-9c7d-1234567890ab/decrypt)

RESPONSE STRUCTURE:
1. Provide a clear, comprehensive answer with natural document references
2. End with relevance assessment:

---
### üìä √âvaluation de la r√©ponse
**Note de pertinence** : [Score]/10  
**Sources consult√©es** : [Number] document(s)

### üìÑ Documents consult√©s
**Complete IDs**: Always use the COMPLETE ID provided in the context (do not truncate)
[List each unique document referenced with download link, no duplicates]
- [filename1.ext]({self.base_url}/api/files/complete_id1/decrypt)
- [filename2.ext]({self.base_url}/api/files/complete_id2/decrypt)

DOCUMENT CONTEXT:
{context_text}

CRITICAL: Use COMPLETE IDs exactly as provided in context - never truncate or modify them!
"""
            
            # Pr√©parer les messages
            messages = []
            
            # Ajouter l'historique de conversation si fourni
            if conversation_history:
                messages.extend(conversation_history[-6:])  # Garder les 6 derniers √©changes
            
            # Ajouter la question actuelle
            messages.append({
                "role": "user",
                "content": user_query
            })
            
            logger.info(f"üåä G√©n√©ration RAG streaming pour: {user_query[:100]}...")
            logger.info(f"üìã Historique conversation: {len(conversation_history) if conversation_history else 0} messages")
            
            # G√©n√©rer la r√©ponse en streaming avec param√®tres configurables
            for chunk in self.generate_response_stream(
                messages=messages,
                system_prompt=system_prompt,
                max_tokens=None,  # Utiliser Config.MAX_TOKENS
                temperature=None,  # Utiliser Config.TEMPERATURE
                top_p=None  # Utiliser Config.TOP_P
            ):
                yield chunk
            
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration de r√©ponse RAG streaming: {e}")
            yield f"Erreur lors de la g√©n√©ration de r√©ponse: {str(e)}"
    
    def _build_context(self, documents: List[Dict[str, Any]]) -> str:
        """
        Construit le texte de contexte √† partir des documents
        
        Args:
            documents: Liste des documents de contexte
            
        Returns:
            str: Contexte format√©
        """
        context_parts = []
        
        for i, doc in enumerate(documents, 1):
            file_name = doc.get("file_name", "Fichier inconnu")
            content = doc.get("content", "")
            file_id = doc.get("file_id", "")
            
            # Utiliser le vrai nom du fichier comme titre principal
            context_part = f"""
Source: {file_name} (ID: {file_id})
Content:
{content}
---
"""
            context_parts.append(context_part)
        
        return "\n".join(context_parts)
    
    def process_response_links(self, response: str, base_url: str) -> str:
        """
        Traite les liens dans la r√©ponse pour les convertir en liens de d√©chiffrement
        
        Args:
            response: R√©ponse du LLM
            base_url: URL de base de l'application
            
        Returns:
            str: R√©ponse avec liens trait√©s
        """
        import re
        
        try:
            # Pattern pour d√©tecter les r√©f√©rences aux fichiers format [FICHIER:id:nom]
            file_pattern = r'\[FICHIER:([^:]+):([^\]]+)\]'
            
            # Pattern pour d√©tecter les liens markdown existants avec file_id (AVEC /decrypt)
            markdown_pattern = r'\[([^\]]+)\]\(' + re.escape(base_url) + r'/api/files/([^/]+)/decrypt\)'
            
            # Pattern pour d√©tecter les liens markdown SANS /decrypt (√† corriger)
            incomplete_pattern = r'\[([^\]]+)\]\(' + re.escape(base_url) + r'/api/files/([^/)]+)\)'
            
            # Chercher tous les matchs
            file_matches = re.findall(file_pattern, response)
            markdown_matches = re.findall(markdown_pattern, response)
            incomplete_matches = re.findall(incomplete_pattern, response)
            
            def replace_file_link(match):
                file_id = match.group(1)
                file_name = match.group(2)
                decrypt_url = f"{base_url}/api/files/{file_id}/decrypt"
                return f'[{file_name}]({decrypt_url})'
                
            def fix_incomplete_link(match):
                file_name = match.group(1)
                file_id = match.group(2)
                decrypt_url = f"{base_url}/api/files/{file_id}/decrypt"
                return f'[{file_name}]({decrypt_url})'
            
            # Remplacer les r√©f√©rences [FICHIER:...] par des liens
            processed_response = re.sub(file_pattern, replace_file_link, response)
            
            # Corriger les liens incomplets (sans /decrypt)
            processed_response = re.sub(incomplete_pattern, fix_incomplete_link, processed_response)
            
            return processed_response
            
        except Exception as e:
            logger.error(f"Erreur lors du traitement des liens: {e}")
            return response
    
    def summarize_document(self, content: str, max_length: int = 500) -> str:
        """
        R√©sume un document
        
        Args:
            content: Contenu du document
            max_length: Longueur maximale du r√©sum√©
            
        Returns:
            str: R√©sum√© du document
        """
        try:
            messages = [{
                "role": "user",
                "content": f"Summarize the following document in maximum {max_length} characters:\n\n{content}"
            }]
            
            system_prompt = "You are an expert in document summarization. Produce clear, concise and informative summaries. Respond in the same language as the input text."
            
            summary = self.generate_response(
                messages=messages,
                system_prompt=system_prompt,
                max_tokens=200,
                temperature=0.3
            )
            
            return summary
            
        except Exception as e:
            logger.error(f"Erreur lors du r√©sum√©: {e}")
            return "R√©sum√© non disponible"
    
    async def generate_rag_response_encrypted(self, 
                                      question: str, 
                                      conversation_history: List[Dict[str, str]] = None,
                                      search_service=None,
                                      embedding_service=None) -> str:
        """
        G√©n√®re une r√©ponse RAG avec recherche chiffr√©e
        
        Args:
            question: Question de l'utilisateur
            conversation_history: Historique de conversation
            search_service: Service de recherche (fallback si pas de recherche chiffr√©e)
            embedding_service: Service d'embeddings
            
        Returns:
            str: R√©ponse g√©n√©r√©e avec contexte d√©chiffr√©
        """
        try:
            if not embedding_service:
                raise ValueError("Service d'embeddings requis pour la recherche")
            
            # Initialiser search_results
            search_results = None
            
            # 1. G√©n√©rer embedding de la question
            question_embedding = embedding_service.generate_embedding(question)
            
            # 2. Recherche chiffr√©e ou normale
            if self.use_encrypted_search and self.encrypted_search_service:
                try:
                    # Chiffrer l'embedding avec la m√™me cl√© que les documents index√©s
                    encryption_context = self.encrypted_search_service.get_search_context()
                    
                    encrypted_query_vector = await self.encrypted_search_service.encrypt_query_vector(
                        question_embedding, 
                        encryption_context
                    )
                    
                    # Recherche sur donn√©es chiffr√©es
                    search_results = await self.encrypted_search_service.search_documents_encrypted(
                        query=question,
                        vector_query=encrypted_query_vector,
                        top=5
                    )
                    logger.info("üîç Recherche sur donn√©es chiffr√©es effectu√©e")
                    
                except Exception as e:
                    logger.error(f"Erreur recherche chiffr√©e: {e}")
                    
                    if not search_service:
                        raise ValueError("Service de recherche requis en fallback")
                    
                    search_results = search_service.search_documents(
                        query=question, 
                        vector_query=question_embedding,
                        top=5
                    )
                
            else:
                if not search_service:
                    raise ValueError("Service de recherche requis en fallback")
                
                search_results = search_service.search_documents(
                    query=question, 
                    vector_query=question_embedding,
                    top=5
                )
            
            # V√©rifier que search_results a √©t√© d√©fini
            if search_results is None:
                raise ValueError("Aucun r√©sultat de recherche obtenu")
            
            # 4. Les r√©sultats sont automatiquement d√©chiffr√©s
            logger.info(f"üìÑ NOMBRE DE DOCUMENTS TROUV√âS: {len(search_results.get('results', []))}")
            
            # 5. Construction du contexte et g√©n√©ration de la r√©ponse (inchang√©e)
            context = self._build_context(search_results["results"])
            
            # Construire l'historique de conversation
            if conversation_history is None:
                conversation_history = []
            
            logger.info(f"üìã HISTORIQUE CONVERSATION: {len(conversation_history)} messages")
            
            # Cr√©er le prompt syst√®me pour RAG chiffr√© optimis√© pour phi3.5:3.8b
            system_prompt = f"""You are a professional AI assistant specialized in document analysis and information retrieval. You provide accurate, well-structured answers based on provided documents.

CORE INSTRUCTIONS:
1. **Source-based answers**: Answer ONLY using information from the provided context documents
2. **Document references**: Reference context documents in your answer.
3. **Complete IDs**: Always use the COMPLETE ID provided in the context (do not truncate)
4. **Language consistency**: Always respond in the same language as the user's question
5. **Document list**: Always end with a complete list of referenced documents

CRITICAL LINK FORMAT:
- Each document in context shows: "Source: filename.ext (ID: complete_file_id)"
- Use the COMPLETE ID exactly as provided
- Format: [filename.ext]({self.base_url}/api/files/complete_file_id/decrypt)
- Example: If context shows "Source: role_library.pdf (ID: ebcc2e60-8a86-4b2f-9c7d-1234567890ab)"
  Write: [role_library.pdf]({self.base_url}/api/files/ebcc2e60-8a86-4b2f-9c7d-1234567890ab/decrypt)

RESPONSE STRUCTURE:
1. Provide a clear, comprehensive answer with natural document references
2. End with relevance assessment:

---
### üìä √âvaluation de la r√©ponse
**Note de pertinence** : [Score]/10  

### üìÑ Documents consult√©s
[List each unique document referenced with download link, check the filename.ext in the "Source:" to ensure no duplicate ]
- [filename1.ext]({self.base_url}/api/files/complete_id1/decrypt)
- [filename2.ext]({self.base_url}/api/files/complete_id2/decrypt)

DOCUMENT CONTEXT:
{context}

CRITICAL: Use COMPLETE IDs exactly as provided in context - never truncate or modify them!
"""
            
            # Pr√©parer les messages
            messages = []
            
            # Ajouter l'historique de conversation si fourni
            if conversation_history:
                messages.extend(conversation_history[-6:])  # Garder les 6 derniers √©changes
            
            # Ajouter la question actuelle
            messages.append({
                "role": "user",
                "content": question
            })
            
            # G√©n√©rer la r√©ponse avec param√®tres configurables EN STREAMING
            full_response = ""
            for chunk in self.generate_response_stream(
                messages=messages,
                system_prompt=system_prompt,
                max_tokens=None,  # Utiliser Config.MAX_TOKENS
                temperature=None,  # Utiliser Config.TEMPERATURE
                top_p=None  # Utiliser Config.TOP_P
            ):
                full_response += chunk
            
            response = full_response
            
            logger.info("‚úÖ FIN G√âN√âRATION RAG CHIFFR√âE")
            logger.info("üîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîçüîç")
            
            return response
            
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration RAG chiffr√©e: {e}")
            raise
    
    def generate_rag_response_encrypted_stream(self, 
                                      question: str, 
                                      conversation_history: List[Dict[str, str]] = None,
                                      search_service=None,
                                      embedding_service=None):
        """
        G√©n√®re une r√©ponse RAG avec recherche chiffr√©e en streaming (version synchrone)
        
        Args:
            question: Question de l'utilisateur
            conversation_history: Historique de conversation
            search_service: Service de recherche (fallback si pas de recherche chiffr√©e)
            embedding_service: Service d'embeddings
            
        Yields:
            str: Fragments de r√©ponse g√©n√©r√©s avec contexte d√©chiffr√©
        """
        try:
            if not embedding_service:
                yield "Erreur: Service d'embeddings requis pour la recherche"
                return
            
            # Initialiser search_results
            search_results = None
            
            # 1. G√©n√©rer embedding de la question
            question_embedding = embedding_service.generate_embedding(question)
            
            # 2. Recherche chiffr√©e ou normale
            if self.use_encrypted_search and self.encrypted_search_service:
                try:
                    # Pour le streaming, on doit simuler async en utilisant la recherche normale
                    # car la recherche chiffr√©e est async mais le streaming est sync
                    logger.info("üîç Fallback vers recherche normale pour streaming chiffr√©")
                    
                    if not search_service:
                        yield "Erreur: Service de recherche requis en fallback"
                        return
                    
                    search_results = search_service.search_documents(
                        query=question, 
                        vector_query=question_embedding,
                        top=5
                    )
                    
                except Exception as e:
                    logger.error(f"Erreur recherche fallback: {e}")
                    
                    if not search_service:
                        yield "Erreur: Service de recherche requis en fallback"
                        return
                    
                    search_results = search_service.search_documents(
                        query=question, 
                        vector_query=question_embedding,
                        top=5
                    )
                
            else:
                if not search_service:
                    yield "Erreur: Service de recherche requis en fallback"
                    return
                
                search_results = search_service.search_documents(
                    query=question, 
                    vector_query=question_embedding,
                    top=5
                )
            
            # V√©rifier que search_results a √©t√© d√©fini
            if search_results is None:
                yield "Erreur: Aucun r√©sultat de recherche obtenu"
                return
            
            # 4. Les r√©sultats sont automatiquement d√©chiffr√©s
            logger.info(f"üìÑ NOMBRE DE DOCUMENTS TROUV√âS: {len(search_results.get('results', []))}")
            
            # 5. Construction du contexte et g√©n√©ration de la r√©ponse
            context = self._build_context(search_results["results"])
            
            # Construire l'historique de conversation
            if conversation_history is None:
                conversation_history = []
            
            logger.info(f"üìã HISTORIQUE CONVERSATION: {len(conversation_history)} messages")
            
            # Cr√©er le prompt syst√®me pour RAG chiffr√© optimis√© pour phi3.5:3.8b
            system_prompt = f"""You are a professional AI assistant specialized in document analysis and information retrieval. You provide accurate, well-structured answers based on provided documents.

CORE INSTRUCTIONS:
1. **Source-based answers**: Answer ONLY using information from the provided context documents
2. **Document references**: Reference context documents in your answer.
3. **Complete IDs**: Always use the COMPLETE ID provided in the context (do not truncate)
4. **Language consistency**: Always respond in the same language as the user's question
5. **Document list**: Always end with a complete list of referenced documents

CRITICAL LINK FORMAT:
- Each document in context shows: "Source: filename.ext (ID: complete_file_id)"
- Use the COMPLETE ID exactly as provided
- Format: [filename.ext]({self.base_url}/api/files/complete_file_id/decrypt)
- Example: If context shows "Source: role_library.pdf (ID: ebcc2e60-8a86-4b2f-9c7d-1234567890ab)"
  Write: [role_library.pdf]({self.base_url}/api/files/ebcc2e60-8a86-4b2f-9c7d-1234567890ab/decrypt)

RESPONSE STRUCTURE:
1. Provide a clear, comprehensive answer with natural document references
2. End with relevance assessment:

---
### üìä √âvaluation de la r√©ponse
**Note de pertinence** : [Score]/10  

### üìÑ Documents consult√©s
[List each unique document referenced with download link, check the filename.ext in the "Source:" to ensure no duplicate ]
- [filename1.ext]({self.base_url}/api/files/complete_id1/decrypt)
- [filename2.ext]({self.base_url}/api/files/complete_id2/decrypt)

DOCUMENT CONTEXT:
{context}

CRITICAL: Use COMPLETE IDs exactly as provided in context - never truncate or modify them!
"""
            
            # Pr√©parer les messages
            messages = []
            
            # Ajouter l'historique de conversation si fourni
            if conversation_history:
                messages.extend(conversation_history[-6:])  # Garder les 6 derniers √©changes
            
            # Ajouter la question actuelle
            messages.append({
                "role": "user",
                "content": question
            })
            
            logger.info("üåä G√©n√©ration RAG chiffr√©e streaming")
            
            # G√©n√©rer la r√©ponse en streaming avec param√®tres configurables
            for chunk in self.generate_response_stream(
                messages=messages,
                system_prompt=system_prompt,
                max_tokens=None,  # Utiliser Config.MAX_TOKENS
                temperature=None,  # Utiliser Config.TEMPERATURE
                top_p=None  # Utiliser Config.TOP_P
            ):
                yield chunk
            
            logger.info("‚úÖ FIN G√âN√âRATION RAG CHIFFR√âE STREAMING")
            
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration RAG chiffr√©e streaming: {e}")
            yield f"Erreur lors de la g√©n√©ration de r√©ponse: {str(e)}"
    
    def extract_keywords(self, content: str, max_keywords: int = 10) -> List[str]:
        """
        Extrait les mots-cl√©s d'un document
        
        Args:
            content: Contenu du document
            max_keywords: Nombre maximum de mots-cl√©s
            
        Returns:
            List[str]: Liste des mots-cl√©s
        """
        try:
            messages = [{
                "role": "user",
                "content": f"Extract {max_keywords} main keywords from the following document. Respond only with the keywords separated by commas:\n\n{content}"
            }]
            
            keywords_response = self.generate_response(
                messages=messages,
                max_tokens=100,
                temperature=0.1
            )
            
            # Parser les mots-cl√©s
            keywords = [kw.strip() for kw in keywords_response.split(",")]
            return keywords[:max_keywords]
            
        except Exception as e:
            logger.error(f"Erreur lors de l'extraction de mots-cl√©s: {e}")
            return []
